{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcd0803",
   "metadata": {},
   "source": [
    "\n",
    "# Confounded vs deconfounded model comparison\n",
    "\n",
    "Train multiple classifiers on a confounded dataset and several pre-generated deconfounded datasets (e.g., backdoor, frontdoor, truncated factorisation). Compare their predictive metrics on a shared observational hold-out and on each deconfounded hold-out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665524c",
   "metadata": {},
   "source": [
    "\n",
    "**Workflow**\n",
    "1. Load confounded CSV and multiple deconfounded CSVs (already generated; no bootstrap here).\n",
    "2. Align feature columns common to all datasets and split each into train/test.\n",
    "3. Fit a set of common models on each training set.\n",
    "4. Evaluate on the same confounded hold-out to see how deconfounding affects generalization to observational data.\n",
    "5. Also evaluate on each deconfounded hold-out for interventional-like performance.\n",
    "\n",
    "> Set DECONFOUNDED_DATASETS below to your deconfounded files (same schema + target).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbb259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import clone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c774b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Paths & columns ===\n",
    "CONFOUNDED_PATH = 'heart_disease_preprocessed.csv'\n",
    "DECONFOUNDED_DATASETS = [\n",
    "    {'name': 'Backdoor', 'path': 'heart_disease_preprocessed_backdoor.csv'},\n",
    "    {'name': 'Frontdoor', 'path': 'heart_disease_preprocessed_frontdoor.csv'},\n",
    "    {'name': 'TruncatedFactorisation', 'path': 'heart_disease_preprocessed_tf.csv'},\n",
    "]\n",
    "\n",
    "TARGET = 'heartdiseasepresence'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "assert Path(CONFOUNDED_PATH).exists(), f\"Confounded file not found: {CONFOUNDED_PATH}\"\n",
    "for ds in DECONFOUNDED_DATASETS:\n",
    "    assert Path(ds['path']).exists(), f\"Deconfounded file not found: {ds['path']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0f0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common features: 25\n",
      "Confounded shape: (272, 26)\n",
      "Deconfounded (Backdoor) shape: (544, 26)\n",
      "Deconfounded (Frontdoor) shape: (544, 26)\n",
      "Deconfounded (TruncatedFactorisation) shape: (544, 26)\n",
      "Confounded target balance: 0    136\n",
      "1    136\n",
      "Name: heartdiseasepresence, dtype: int64\n",
      "Deconfounded (Backdoor) target balance: 0    272\n",
      "1    272\n",
      "Name: heartdiseasepresence, dtype: int64\n",
      "Deconfounded (Frontdoor) target balance: 0    272\n",
      "1    272\n",
      "Name: heartdiseasepresence, dtype: int64\n",
      "Deconfounded (TruncatedFactorisation) target balance: 0    272\n",
      "1    272\n",
      "Name: heartdiseasepresence, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ca</th>\n",
       "      <th>chol</th>\n",
       "      <th>cp_Asymptomatic</th>\n",
       "      <th>cp_AtypicalAngina</th>\n",
       "      <th>cp_NonAnginalPain</th>\n",
       "      <th>cp_TypicalAngina</th>\n",
       "      <th>exang_NoExAngina</th>\n",
       "      <th>exang_YesExAngina</th>\n",
       "      <th>fbs_&lt;=120</th>\n",
       "      <th>...</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>slope_Downsloping</th>\n",
       "      <th>slope_Flat</th>\n",
       "      <th>slope_Upsloping</th>\n",
       "      <th>thal_FixedDefect</th>\n",
       "      <th>thal_Normal</th>\n",
       "      <th>thal_ReversibleDefect</th>\n",
       "      <th>thalach</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>heartdiseasepresence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950883</td>\n",
       "      <td>-0.740979</td>\n",
       "      <td>-0.289108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040935</td>\n",
       "      <td>0.743598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.397584</td>\n",
       "      <td>2.527338</td>\n",
       "      <td>0.785340</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.757678</td>\n",
       "      <td>1.593663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.397584</td>\n",
       "      <td>1.437899</td>\n",
       "      <td>-0.370199</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.858371</td>\n",
       "      <td>-0.673176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.952676</td>\n",
       "      <td>-0.740979</td>\n",
       "      <td>0.055526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.625427</td>\n",
       "      <td>-0.106466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.505975</td>\n",
       "      <td>-0.740979</td>\n",
       "      <td>-0.877014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983065</td>\n",
       "      <td>-0.106466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age        ca      chol  cp_Asymptomatic  cp_AtypicalAngina  \\\n",
       "0  0.950883 -0.740979 -0.289108                0                  0   \n",
       "1  1.397584  2.527338  0.785340                1                  0   \n",
       "2  1.397584  1.437899 -0.370199                1                  0   \n",
       "3 -1.952676 -0.740979  0.055526                0                  0   \n",
       "4 -1.505975 -0.740979 -0.877014                0                  1   \n",
       "\n",
       "   cp_NonAnginalPain  cp_TypicalAngina  exang_NoExAngina  exang_YesExAngina  \\\n",
       "0                  0                 1                 1                  0   \n",
       "1                  0                 0                 0                  1   \n",
       "2                  0                 0                 0                  1   \n",
       "3                  1                 0                 1                  0   \n",
       "4                  0                 0                 1                  0   \n",
       "\n",
       "   fbs_<=120  ...  sex_Male  slope_Downsloping  slope_Flat  slope_Upsloping  \\\n",
       "0          0  ...         1                  1           0                0   \n",
       "1          1  ...         1                  0           1                0   \n",
       "2          1  ...         1                  0           1                0   \n",
       "3          1  ...         1                  1           0                0   \n",
       "4          1  ...         0                  0           0                1   \n",
       "\n",
       "   thal_FixedDefect  thal_Normal  thal_ReversibleDefect   thalach  trestbps  \\\n",
       "0                 1            0                      0  0.040935  0.743598   \n",
       "1                 0            1                      0 -1.757678  1.593663   \n",
       "2                 0            0                      1 -0.858371 -0.673176   \n",
       "3                 0            1                      0  1.625427 -0.106466   \n",
       "4                 0            1                      0  0.983065 -0.106466   \n",
       "\n",
       "   heartdiseasepresence  \n",
       "0                     0  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here I am doing basic checks to ensure that all datasets have the same columns and target distributions\n",
    "# Load datasets\n",
    "df_conf = pd.read_csv(CONFOUNDED_PATH)\n",
    "deconf_dfs = {}\n",
    "for ds in DECONFOUNDED_DATASETS:\n",
    "    deconf_dfs[ds['name']] = pd.read_csv(ds['path'])\n",
    "\n",
    "# Align columns across all datasets (intersection to ensure comparability)\n",
    "common_cols = set(df_conf.columns)\n",
    "for name, df in deconf_dfs.items():\n",
    "    common_cols = common_cols.intersection(set(df.columns))\n",
    "assert TARGET in common_cols, f\"Target '{TARGET}' must exist in all datasets\"\n",
    "\n",
    "feature_cols = sorted([c for c in common_cols if c != TARGET])\n",
    "\n",
    "df_conf = df_conf[feature_cols + [TARGET]]\n",
    "for name in deconf_dfs:\n",
    "    deconf_dfs[name] = deconf_dfs[name][feature_cols + [TARGET]]\n",
    "\n",
    "print('Common features:', len(feature_cols))\n",
    "print('Confounded shape:', df_conf.shape)\n",
    "for name, df in deconf_dfs.items():\n",
    "    print(f\"Deconfounded ({name}) shape: {df.shape}\")\n",
    "\n",
    "print('Confounded target balance:', df_conf[TARGET].value_counts())\n",
    "for name, df in deconf_dfs.items():\n",
    "    print(f\"Deconfounded ({name}) target balance:\", df[TARGET].value_counts())\n",
    "\n",
    "df_conf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eb8c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confounded train/test sizes: 217 55\n",
      "Backdoor train/test sizes: 435 109\n",
      "Frontdoor train/test sizes: 435 109\n",
      "TruncatedFactorisation train/test sizes: 435 109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test splits for each dataset\n",
    "splits = {\n",
    "    'Confounded': {\n",
    "        'train': None,\n",
    "        'test': None,\n",
    "        'label': 'Confounded (observational)'\n",
    "    }\n",
    "}\n",
    "\n",
    "conf_train, conf_test = train_test_split(\n",
    "    df_conf, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=df_conf[TARGET]\n",
    ")\n",
    "splits['Confounded']['train'] = conf_train\n",
    "splits['Confounded']['test'] = conf_test\n",
    "\n",
    "for name, df in deconf_dfs.items():\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=df[TARGET]\n",
    "    )\n",
    "    splits[name] = {'train': train_df, 'test': test_df, 'label': f'Deconfounded ({name})'}\n",
    "\n",
    "for name, split in splits.items():\n",
    "    print(name, 'train/test sizes:', len(split['train']), len(split['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dd7f3",
   "metadata": {},
   "source": [
    "\n",
    "### Models and evaluation helpers\n",
    "\n",
    "Metrics: Accuracy, ROC AUC, PR AUC (average precision), Brier score, and log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312e4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=2000, penalty='l2', C=0.1, solver='liblinear', random_state=RANDOM_SEED),\n",
    "    'RandomForest': RandomForestClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100, random_state=RANDOM_SEED),\n",
    "    'SVC': SVC(kernel='linear', C=1.0, gamma='scale', random_state=RANDOM_SEED),\n",
    "    'MLPClassifier': MLPClassifier(\n",
    "        activation='relu',\n",
    "        alpha=0.0001,\n",
    "        hidden_layer_sizes=(50,),\n",
    "        learning_rate='constant',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        colsample_bytree=1.0,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        n_estimators=300,\n",
    "        subsample=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "def get_probas(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, 'decision_function'):\n",
    "        scores = model.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-scores))\n",
    "    raise ValueError('Model does not support probability-like outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate all train/eval combinations\n",
    "results = []\n",
    "\n",
    "# Evaluation sets: always include the confounded test, plus each deconfounded test\n",
    "eval_sets = [('Confounded', splits['Confounded']['test'])]\n",
    "for name, split in splits.items():\n",
    "    if name == 'Confounded':\n",
    "        continue\n",
    "    eval_sets.append((name, split['test']))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for train_name, split in splits.items():\n",
    "        X_train = split['train'][feature_cols]\n",
    "        y_train = split['train'][TARGET]\n",
    "\n",
    "        for eval_name, eval_df in eval_sets:\n",
    "            X_eval = eval_df[feature_cols]\n",
    "            y_eval = eval_df[TARGET]\n",
    "            metrics = evaluate_model(model, X_train, y_train, X_eval, y_eval)\n",
    "            metrics.update({\n",
    "                'model': model_name,\n",
    "                'training_data': splits[train_name]['label'],\n",
    "                'evaluation': 'Confounded test' if eval_name == 'Confounded' else f'Deconfounded test ({eval_name})'\n",
    "            })\n",
    "            results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d4d54",
   "metadata": {},
   "source": [
    "\n",
    "### Combined, tidy comparison table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tidy(df):\n",
    "    cols = ['model', 'training_data', 'evaluation', 'accuracy', 'roc_auc', 'pr_auc', 'brier', 'log_loss']\n",
    "    return df[cols].sort_values(['evaluation', 'model', 'training_data']).reset_index(drop=True)\n",
    "\n",
    "tidy(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce227486",
   "metadata": {},
   "source": [
    "\n",
    "### Visualize observational (confounded) test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65999261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = results_df[results_df['evaluation'] == 'Confounded test'].copy()\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "train_labels = plot_df['training_data'].unique()\n",
    "train_labels_sorted = sorted(train_labels)\n",
    "x_pos = np.arange(len(train_labels_sorted))\n",
    "\n",
    "width = 0.15\n",
    "for idx, model_name in enumerate(sorted(plot_df['model'].unique())):\n",
    "    subset = plot_df[plot_df['model'] == model_name].set_index('training_data').loc[train_labels_sorted]\n",
    "    bars = ax.bar(x_pos + idx * width, subset['accuracy'], width=width, label=model_name)\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005, f\"{bar.get_height():.3f}\",\n",
    "                ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "ax.set_xticks(x_pos + width * (len(plot_df['model'].unique()) - 1) / 2)\n",
    "ax.set_xticklabels(train_labels_sorted, rotation=15, ha='right')\n",
    "ax.set_ylabel('Accuracy on confounded test')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
